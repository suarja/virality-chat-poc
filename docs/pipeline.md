# TikTok Virality Analysis Pipeline

## Overview

This pipeline analyzes TikTok videos to predict and understand virality factors through three main phases with advanced batch processing:

1. **Data Collection**: Scrape TikTok videos using Apify API (batch processing)
2. **Content Analysis**: Analyze videos using Google's Gemini AI (batch processing)
3. **Feature Engineering**: Extract and combine features for modeling (batch processing)

## ğŸ†• New Batch Processing System

### Key Features

- **Configurable Batch Sizes**: Process 1-10 accounts per batch
- **Progress Tracking**: Automatic tracking in `source.txt`
- **Error Management**: Detailed error logging in `errors.txt`
- **Resume Capability**: Continue from where it left off
- **Retry System**: Retry failed accounts or specific phases
- **Dataset Versioning**: Multiple dataset versions with independent tracking

### Architecture

```
Pipeline Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Batch Input   â”‚â”€â”€â”€â–¶â”‚  Phase 1:       â”‚â”€â”€â”€â–¶â”‚  Phase 2:       â”‚
â”‚   (N accounts)  â”‚    â”‚  Scraping       â”‚    â”‚  Gemini         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Save Raw Data  â”‚    â”‚  Save Analysis  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                               â”‚  Phase 3:       â”‚
                                               â”‚  Features       â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                               â”‚  Mark Complete  â”‚
                                               â”‚  Update source  â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Pipeline Components

### 1. Batch Tracker (`src/utils/batch_tracker.py`)

**Core Functionality:**

- Track processed accounts in `source.txt`
- Log errors by phase in `errors.txt`
- Provide batch management utilities
- Generate progress summaries

**Key Methods:**

```python
tracker = BatchTracker("dataset_name")
tracker.get_next_batch(accounts, batch_size)
tracker.mark_account_processed(account)
tracker.log_error(account, phase, error)
tracker.summarize_progress()
```

### 2. TikTok Scraping (`src/scraping/`)

- **Main Script**: Integrated into `run_pipeline.py`
- **Implementation**: `src/scraping/tiktok_scraper.py`
- **Configuration**: Set `APIFY_API_TOKEN` in `.env`
- **Output**: `data/raw/dataset_*/batch_*.json`

**Features:**

- Profile and video metadata collection
- Configurable video count per account
- Rate limiting and error handling
- Batch-based processing

### 3. Gemini Analysis (`scripts/test_gemini.py`)

- **Configuration**: Set `GOOGLE_API_KEY` in `.env`
- **Output**: `data/analysis/dataset_*/gemini_*.json`

**Analysis Components:**

- Visual elements and style
- Content structure and flow
- Engagement factors
- Technical elements
- Trend alignment
- Improvement suggestions

### 4. Feature Engineering (`src/features/`)

- **Main Script**: Integrated into `run_pipeline.py`
- **Components**:
  - `feature_extractor.py`: Basic feature extraction
  - `data_processor.py`: Combine and process data
- **Output**: `data/features/dataset_*/features.csv`

**Features Extracted:**

- Basic engagement metrics
- Engagement ratios
- Content features
- Temporal patterns
- AI-powered analysis features

## Running the Pipeline

### Basic Usage

```bash
# Run with default settings
python scripts/run_pipeline.py --dataset v1

# Custom batch size
python scripts/run_pipeline.py --dataset v1 --batch-size 3

# Custom video limits
python scripts/run_pipeline.py --dataset v1 --videos-per-account 10 --max-total-videos 300
```

### Advanced Options

```bash
# Retry failed accounts
python scripts/run_pipeline.py --dataset v1 --retry-failed

# Retry specific phase
python scripts/run_pipeline.py --dataset v1 --retry-failed --retry-phase analysis

# Test run
python scripts/run_pipeline.py --dataset test --batch-size 1 --videos-per-account 2
```

### Command Line Arguments

| Argument               | Type | Default  | Description                   |
| ---------------------- | ---- | -------- | ----------------------------- |
| `--dataset`            | str  | Required | Dataset name (e.g., v1, test) |
| `--batch-size`         | int  | 5        | Accounts per batch            |
| `--videos-per-account` | int  | 15       | Videos per account            |
| `--max-total-videos`   | int  | 500      | Maximum total videos          |
| `--retry-failed`       | flag | False    | Retry failed accounts         |
| `--retry-phase`        | str  | None     | Specific phase to retry       |

## Directory Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraping/
â”‚   â”‚   â”œâ”€â”€ tiktok_scraper.py
â”‚   â”‚   â””â”€â”€ data_validator.py
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ feature_extractor.py
â”‚   â”‚   â””â”€â”€ data_processor.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ batch_tracker.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_pipeline.py
â”‚   â”œâ”€â”€ test_batch_system.py
â”‚   â””â”€â”€ test_gemini.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset_v1/
â”‚   â”‚   â”œâ”€â”€ source.txt          # Processed accounts
â”‚   â”‚   â”œâ”€â”€ errors.txt          # Error tracking
â”‚   â”‚   â””â”€â”€ metadata.json       # Dataset metadata
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ dataset_v1/
â”‚   â”‚       â””â”€â”€ batch_*.json    # Consolidated video data
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â””â”€â”€ dataset_v1/
â”‚   â”‚       â””â”€â”€ gemini_*.json   # AI analysis results
â”‚   â””â”€â”€ features/
â”‚       â””â”€â”€ dataset_v1/
â”‚           â””â”€â”€ features.csv    # Final feature dataset
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ pipeline_v1.log         # Pipeline logs
â”‚   â””â”€â”€ errors.log              # Error logs
â””â”€â”€ docs/
    â””â”€â”€ gemini_analysis/        # Legacy analysis files
```

## Configuration

1. Create `.env` file:

```bash
APIFY_API_TOKEN=your_apify_token
GOOGLE_API_KEY=your_gemini_key
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

## Output Files

### 1. Dataset Tracking (`data/dataset_*/`)

- **source.txt**: List of successfully processed accounts
- **errors.txt**: Error details by account and phase
- **metadata.json**: Dataset configuration and statistics

### 2. Raw TikTok Data (`data/raw/dataset_*/`)

- **batch\_\*.json**: Consolidated video data per batch
- Format: `batch_YYYYMMDD_HHMMSS.json`

### 3. Gemini Analysis (`data/analysis/dataset_*/`)

- **gemini\_\*.json**: Analysis results per video
- Format: `gemini_video_id_analysis.json`

### 4. Processed Features (`data/features/dataset_*/`)

- **features.csv**: Consolidated feature dataset
- **metadata.json**: Feature extraction metadata

## Error Handling

The pipeline includes comprehensive error handling:

- **Phase-specific error tracking**
- **Automatic retry capabilities**
- **Detailed error logging**
- **Progress preservation**
- **Batch-level error isolation**

### Error Recovery

```bash
# Check for errors
cat data/dataset_v1/errors.txt

# Retry all failed accounts
python scripts/run_pipeline.py --dataset v1 --retry-failed

# Retry only analysis phase
python scripts/run_pipeline.py --dataset v1 --retry-failed --retry-phase analysis
```

## Monitoring

Monitor pipeline progress through:

1. **Console output** with emoji indicators
2. **Detailed logs** in `logs/pipeline_*.log`
3. **Progress tracking** in `source.txt`
4. **Error monitoring** in `errors.txt`
5. **Summary statistics** after each batch

### Progress Commands

```bash
# Check processing status
cat data/dataset_v1/source.txt

# Monitor logs in real-time
tail -f logs/pipeline_v1.log

# Check error count
wc -l data/dataset_v1/errors.txt
```

## Quality Control

### Data Validation

- **Minimum views threshold**: 1,000 views
- **Maximum video age**: 6 months
- **Required fields completeness**
- **Data type validation**
- **Range checks**

### Feature Validation

- **Completeness checks** (null values)
- **Range validation** (outliers)
- **Cross-feature correlation analysis**
- **Temporal consistency**

## Next Steps

### 1. Feature Enhancement

- [ ] Add audio analysis features
- [ ] Implement trend detection
- [ ] Add cross-platform metrics
- [ ] Enhance visual analysis

### 2. Pipeline Optimization

- [ ] Add parallel processing
- [ ] Implement caching
- [ ] Add incremental processing
- [ ] Optimize memory usage

### 3. Model Development

- [ ] Implement baseline models
- [ ] Add feature selection
- [ ] Create ensemble models
- [ ] Add interpretability tools

### 4. Monitoring & Alerting

- [ ] Add performance metrics
- [ ] Implement alerting system
- [ ] Add dashboard
- [ ] Create automated reports
