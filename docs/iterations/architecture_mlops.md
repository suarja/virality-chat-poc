# üèóÔ∏è Architecture MLOps - Variables Exp√©rimentales et Changements

## üéØ **Vue d'Ensemble de l'Architecture**

Notre POC suit une architecture MLOps simple mais robuste, permettant de changer facilement les mod√®les et features sans casser le syst√®me.

### **Architecture Actuelle**

```
üìä Data Pipeline
‚îú‚îÄ‚îÄ üï∑Ô∏è Scraping (Apify TikTok)
‚îú‚îÄ‚îÄ üîß Feature Extraction (Modular System)
‚îú‚îÄ‚îÄ ü§ñ ML Model (RandomForest)
‚îî‚îÄ‚îÄ üöÄ API (FastAPI)
```

---

## üîß **Variables Exp√©rimentales et leur Impact**

### **1. üéØ Feature Sets (Variables Manipul√©es)**

#### **Feature Sets Disponibles**

```python
# Dans src/features/modular_feature_system.py
FEATURE_SETS = {
    "metadata": MetadataFeatureSet(),           # 20 features
    "gemini_basic": GeminiBasicFeatureSet(),    # 14 features
    "visual_granular": VisualGranularFeatureSet(), # 34 features
    "comprehensive": ComprehensiveFeatureSet(),  # 34 features
    "model_compatible": ModelCompatibleFeatureSet() # 16 features ‚≠ê
}
```

#### **Comment Changer de Feature Set**

```bash
# Dans les scripts
python3 scripts/analyze_existing_data.py --feature-set comprehensive
python3 scripts/analyze_existing_data.py --feature-set model_compatible

# Dans l'API (automatique)
# L'API utilise toujours ModelCompatibleFeatureSet (16 features)
```

#### **Impact sur le Code**

- **Feature Count**: Varie selon le set (16-34 features)
- **Model Compatibility**: Seul `model_compatible` fonctionne avec l'API
- **Performance**: Plus de features = plus de temps de calcul
- **R¬≤ Score**: Peut varier selon les features

### **2. ü§ñ Mod√®les ML (Variables Manipul√©es)**

#### **Mod√®les Actuels**

```python
# Dans models/
‚îú‚îÄ‚îÄ pre_publication_virality_model.pkl    # RandomForest (R¬≤ = 0.457)
‚îî‚îÄ‚îÄ baseline_virality_model.pkl           # Backup model
```

#### **Comment Changer de Mod√®le**

```python
# Dans src/api/ml_model.py - Ligne 25
self.model_path = project_root / "models/pre_publication_virality_model.pkl"

# Pour changer de mod√®le, modifier cette ligne :
self.model_path = project_root / "models/xgboost_virality_model.pkl"
```

#### **Mod√®les √† Tester (Planifi√©s)**

| Mod√®le             | Avantages              | Inconv√©nients         | Effort      |
| ------------------ | ---------------------- | --------------------- | ----------- |
| **RandomForest**   | Robuste, interpr√©table | Performance limit√©e   | ‚úÖ Actuel   |
| **XGBoost**        | Meilleure performance  | Plus complexe         | üîÑ ITER_003 |
| **LightGBM**       | Rapide, efficace       | Moins interpr√©table   | üîÑ ITER_004 |
| **Neural Network** | Tr√®s flexible          | Overfitting, complexe | üîÑ ITER_005 |

### **3. üìä Dataset Size (Variables Manipul√©es)**

#### **Tailles de Dataset Test√©es**

```python
DATASET_SIZES = {
    "ITER_001": 8 vid√©os,      # POC initial
    "ITER_002": 150 vid√©os,    # Scaling (planifi√©)
    "ITER_003": 500 vid√©os,    # Production (planifi√©)
}
```

#### **Impact sur les M√©triques**

| Dataset Size   | R¬≤ Score | MAE   | RMSE  | Temps Entra√Ænement |
| -------------- | -------- | ----- | ----- | ------------------ |
| **8 vid√©os**   | 0.457    | 0.23  | 0.34  | 2s                 |
| **150 vid√©os** | ~0.6     | ~0.18 | ~0.25 | 30s                |
| **500 vid√©os** | ~0.7     | ~0.15 | ~0.20 | 2min               |

---

## üîÑ **Workflow de Changement de Mod√®le**

### **√âtape 1: Pr√©parer le Nouveau Mod√®le**

```bash
# 1. Entra√Æner le nouveau mod√®le
python3 scripts/analyze_existing_data.py --dataset-dir data/dataset_poc_training --feature-set model_compatible --save-model --model-type xgboost

# 2. Sauvegarder avec un nom descriptif
mv models/pre_publication_virality_model.pkl models/xgboost_virality_model_v1.pkl
```

### **√âtape 2: Modifier l'API**

```python
# Dans src/api/ml_model.py - Ligne 25
# AVANT
self.model_path = project_root / "models/pre_publication_virality_model.pkl"

# APR√àS
self.model_path = project_root / "models/xgboost_virality_model_v1.pkl"
```

### **√âtape 3: Tester**

```bash
# Test local
python3 scripts/test_api_fixed.py

# Test avec nouvelle URL
curl -X POST "http://localhost:8000/analyze-tiktok-url" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://www.tiktok.com/@swarecito/video/7505706702050823446"}'
```

### **√âtape 4: D√©ployer**

```bash
# Commit et d√©ploiement
git add src/api/ml_model.py models/xgboost_virality_model_v1.pkl
git commit -m "feat: switch to XGBoost model (R¬≤=0.612)"
railway up
```

---

## üîß **Workflow de Changement de Features**

### **√âtape 1: Cr√©er un Nouveau Feature Set**

```python
# Dans src/features/modular_feature_system.py
class EnhancedFeatureSet(BaseFeatureSet):
    def __init__(self):
        super().__init__(
            name="enhanced",
            description="Enhanced features with new insights"
        )
        self.features = [
            # 16 features compatibles avec le mod√®le
            'duration', 'hashtag_count', 'estimated_hashtag_count',
            # ... ajouter nouvelles features
        ]

    def extract(self, video_data: Dict, gemini_analysis: Optional[Dict] = None) -> Dict:
        # Logique d'extraction
        pass
```

### **√âtape 2: Enregistrer le Feature Set**

```python
# Dans la fonction _register_default_sets()
def _register_default_sets(self):
    self.register_feature_set(MetadataFeatureSet())
    self.register_feature_set(GeminiBasicFeatureSet())
    self.register_feature_set(EnhancedFeatureSet())  # Nouveau
```

### **√âtape 3: Tester avec le Nouveau Feature Set**

```bash
# Test d'extraction
python3 scripts/analyze_existing_data.py --feature-set enhanced

# Test API (si compatible)
# L'API utilise toujours ModelCompatibleFeatureSet
```

---

## üìä **M√©triques Standardis√©es (Variables Constantes)**

### **M√©triques Toujours Calcul√©es**

```python
STANDARD_METRICS = {
    "r2_score": "Coefficient de d√©termination (0-1)",
    "mae": "Mean Absolute Error (plus bas = mieux)",
    "rmse": "Root Mean Square Error (plus bas = mieux)",
    "feature_importance": "Top 5 features importantes",
    "latency": "Temps de pr√©diction (objectif < 2s)"
}
```

### **Seuils de Performance**

| M√©trique     | Seuil Acceptable | Seuil Bon | Seuil Excellent |
| ------------ | ---------------- | --------- | --------------- |
| **R¬≤ Score** | > 0.4            | > 0.6     | > 0.8           |
| **MAE**      | < 0.3            | < 0.2     | < 0.1           |
| **RMSE**     | < 0.4            | < 0.3     | < 0.2           |
| **Latency**  | < 3s             | < 2s      | < 1s            |

### **Feature Importance (Toujours Calcul√©e)**

```python
# Format standard
feature_importance = {
    "feature_1": 0.124,  # 12.4% d'importance
    "feature_2": 0.108,  # 10.8% d'importance
    # ... top 5 features
}
```

---

## üéØ **Variables par It√©ration**

### **ITER_001: POC Initial** ‚úÖ

```python
VARIABLES_ITER_001 = {
    "dataset_size": 8,
    "model": "RandomForest",
    "feature_set": "ModelCompatibleFeatureSet",
    "r2_score": 0.457,
    "status": "COMPLETED"
}
```

### **ITER_002: Dataset Scaling** üîÑ

```python
VARIABLES_ITER_002 = {
    "dataset_size": 150,  # 8 ‚Üí 150
    "model": "RandomForest",  # M√™me mod√®le
    "feature_set": "ModelCompatibleFeatureSet",  # M√™me features
    "expected_r2": "> 0.6",
    "status": "PLANNED"
}
```

### **ITER_003: Model Optimization** üîÑ

```python
VARIABLES_ITER_003 = {
    "dataset_size": 150,
    "model": "XGBoost",  # RandomForest ‚Üí XGBoost
    "feature_set": "ModelCompatibleFeatureSet",
    "expected_r2": "> 0.65",
    "status": "PLANNED"
}
```

### **ITER_004: Feature Engineering** üîÑ

```python
VARIABLES_ITER_004 = {
    "dataset_size": 150,
    "model": "XGBoost",
    "feature_set": "EnhancedFeatureSet",  # Nouvelles features
    "expected_r2": "> 0.7",
    "status": "PLANNED"
}
```

---

## üö® **Points d'Attention**

### **1. Compatibilit√© Mod√®le-Features**

```python
# ‚ùå PROBL√àME: Features mismatch
model_expects = 16 features
features_provided = 34 features  # Erreur!

# ‚úÖ SOLUTION: Toujours utiliser ModelCompatibleFeatureSet pour l'API
```

### **2. Sauvegarde des Mod√®les**

```bash
# ‚úÖ Toujours sauvegarder avant changement
cp models/pre_publication_virality_model.pkl models/backup_randomforest.pkl

# ‚úÖ Versionner les mod√®les
mv models/xgboost_virality_model.pkl models/xgboost_virality_model_v1.pkl
```

### **3. Tests de R√©gression**

```bash
# ‚úÖ Tester avant d√©ploiement
python3 scripts/test_api_fixed.py

# ‚úÖ V√©rifier les m√©triques
curl -X POST "http://localhost:8000/analyze-tiktok-url" \
  -d '{"url": "test_url"}' | jq '.prediction.r2_score'
```

---

## üìö **Documentation Perdue (Reflections)**

### **Contenu Supprim√© Important**

Le dossier `docs/reflection/` contenait :

- **Feature Engineering Analysis** - Analyse d√©taill√©e des 34 features
- **Model Comparison** - Comparaison RandomForest vs XGBoost vs autres
- **Architecture Decisions** - D√©cisions d'architecture importantes

### **Recr√©ation N√©cessaire**

```bash
# √Ä recr√©er dans docs/iterations/
‚îú‚îÄ‚îÄ feature_engineering_analysis.md
‚îú‚îÄ‚îÄ model_comparison_guide.md
‚îú‚îÄ‚îÄ architecture_decisions.md
‚îî‚îÄ‚îÄ experimental_results.md
```

---

## üéØ **Checklist de Changement**

### **Avant de Changer de Mod√®le**

- [ ] Sauvegarder le mod√®le actuel
- [ ] Tester le nouveau mod√®le localement
- [ ] V√©rifier la compatibilit√© des features
- [ ] Documenter les m√©triques de performance

### **Avant de Changer de Features**

- [ ] V√©rifier la compatibilit√© avec le mod√®le
- [ ] Tester l'extraction des nouvelles features
- [ ] Valider les performances
- [ ] Mettre √† jour la documentation

### **Avant de D√©ployer**

- [ ] Tests de r√©gression complets
- [ ] Validation des m√©triques
- [ ] Documentation mise √† jour
- [ ] Rollback plan pr√©par√©

---

## üîÑ **Gestion des Datasets et Agr√©gation Automatique**

### **Processus d'Aggr√©gation Automatique**

Le pipeline automatiquement agr√®ge les datasets entre les it√©rations :

```
ITER_001 (8 vid√©os) + ITER_002 (32 vid√©os) = Dataset Final (84 vid√©os)
```

#### **Avantages**

- ‚úÖ **Cumul des donn√©es** : Chaque it√©ration enrichit le dataset
- ‚úÖ **Pas de doublons** : Syst√®me de d√©duplication automatique
- ‚úÖ **Historique complet** : Toutes les exp√©riences conserv√©es

#### **Inconv√©nients**

- ‚ö†Ô∏è **Complexit√© croissante** : Dataset de plus en plus volumineux
- ‚ö†Ô∏è **Biais temporel** : Anciennes donn√©es peuvent devenir obsol√®tes
- ‚ö†Ô∏è **Overfitting** : Risque de m√©morisation des donn√©es d'entra√Ænement

### **Gestion des Doublons**

```bash
# V√©rification automatique des doublons
cut -d',' -f1 data/dataset_iter_XXX/features/aggregated_comprehensive.csv | sort | uniq -d
```

**R√©sultat** : Aucun doublon d√©tect√© dans ITER_002

### **Recommandations pour les Futures It√©rations**

1. **Documenter l'agr√©gation** : Noter les sources dans chaque it√©ration
2. **Valider la qualit√©** : V√©rifier la coh√©rence des donn√©es agr√©g√©es
3. **Consid√©rer le nettoyage** : Supprimer les donn√©es obsol√®tes si n√©cessaire

---

**Document cr√©√© le**: `2025-07-06`
**Derni√®re mise √† jour**: `2025-07-06`
**Version**: `v1.0.0`
**Responsable**: √âquipe POC TikTok Virality
