{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üîç Evaluation Framework Demo\n",
        "\n",
        "This notebook demonstrates how to use the evaluation framework to:\n",
        "1. Evaluate feature extraction quality\n",
        "2. Evaluate model predictions\n",
        "3. Generate evaluation reports\n",
        "4. Track metrics with MLflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "# Import evaluation modules\n",
        "from config.evaluation_config import get_config\n",
        "from features.evaluation import evaluate_feature_extraction\n",
        "from models.evaluation import evaluate_predictions\n",
        "from utils.mlflow_utils import get_tracker\n",
        "from utils.report_utils import get_report_generator\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, let's set up our evaluation configuration and utilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get configuration\n",
        "config = get_config()\n",
        "\n",
        "# Initialize MLflow tracker\n",
        "tracker = get_tracker(experiment_name=\"evaluation_demo\")\n",
        "\n",
        "# Initialize report generator\n",
        "report_gen = get_report_generator(mlflow_tracker=tracker)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Feature Extraction Evaluation\n",
        "\n",
        "Let's evaluate the quality of our feature extraction process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sample extracted features\n",
        "import pandas as pd\n",
        "features_df = pd.read_csv(project_root / \"data/processed/extracted_features.csv\")\n",
        "\n",
        "# Evaluate feature extraction\n",
        "with tracker:\n",
        "    feature_eval_results = evaluate_feature_extraction(\n",
        "        features_df,\n",
        "        expected_schema=config.feature_extraction.required_fields\n",
        "    )\n",
        "\n",
        "print(\"Feature Extraction Evaluation Results:\")\n",
        "print(f\"Completeness: {feature_eval_results['completeness']:.2%}\")\n",
        "print(f\"Accuracy: {feature_eval_results['accuracy']:.2%}\")\n",
        "print(f\"Average Latency: {feature_eval_results['avg_latency_ms']:.2f}ms\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Model Prediction Evaluation\n",
        "\n",
        "Now let's evaluate our model's prediction performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test data\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Simulate predictions (replace with actual model predictions)\n",
        "y_true = np.random.randint(0, 2, size=100)\n",
        "y_pred = np.random.randint(0, 2, size=100)\n",
        "\n",
        "# Evaluate predictions\n",
        "with tracker:\n",
        "    pred_eval_results = evaluate_predictions(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        model_metadata={\"name\": \"virality_predictor_v1\"}\n",
        "    )\n",
        "\n",
        "print(\"\\nPrediction Evaluation Results:\")\n",
        "print(f\"Accuracy: {pred_eval_results['accuracy']:.2%}\")\n",
        "print(f\"Precision: {pred_eval_results['precision']:.2%}\")\n",
        "print(f\"Recall: {pred_eval_results['recall']:.2%}\")\n",
        "print(f\"F1 Score: {pred_eval_results['f1']:.2%}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Generate Evaluation Report\n",
        "\n",
        "Let's generate a comprehensive evaluation report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine results for report\n",
        "report_path = report_gen.generate_report(\n",
        "    evaluation_type=\"full_system\",\n",
        "    metrics={\n",
        "        **feature_eval_results,\n",
        "        **pred_eval_results\n",
        "    },\n",
        "    component_name=\"virality_predictor\",\n",
        "    version=\"1.0.0\",\n",
        "    dataset_size=len(y_true),\n",
        "    environment=\"development\",\n",
        "    data_quality={\n",
        "        \"missing_rate\": feature_eval_results[\"missing_rate\"],\n",
        "        \"invalid_rate\": feature_eval_results[\"invalid_rate\"]\n",
        "    },\n",
        "    performance_metrics={\n",
        "        \"feature_extraction_latency_ms\": feature_eval_results[\"avg_latency_ms\"],\n",
        "        \"prediction_latency_ms\": pred_eval_results[\"avg_inference_time_ms\"]\n",
        "    },\n",
        "    error_analysis={\n",
        "        \"feature_errors\": feature_eval_results[\"error_details\"],\n",
        "        \"prediction_errors\": pred_eval_results[\"error_analysis\"]\n",
        "    },\n",
        "    recommendations=[\n",
        "        \"Improve feature completeness for 'hashtags' field\",\n",
        "        \"Optimize prediction latency\",\n",
        "        \"Add more training data for rare categories\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"\\nEvaluation report generated at: {report_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. View MLflow Dashboard\n",
        "\n",
        "To view the tracked metrics and artifacts in MLflow:\n",
        "\n",
        "1. Open a terminal\n",
        "2. Navigate to the project root\n",
        "3. Run: `mlflow ui`\n",
        "4. Open http://localhost:5000 in your browser\n",
        "\n",
        "You'll see:\n",
        "- Feature extraction metrics\n",
        "- Model performance metrics\n",
        "- Generated reports\n",
        "- Trend visualizations\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Continuous Evaluation\n",
        "\n",
        "For continuous evaluation:\n",
        "\n",
        "1. Import this notebook as a Python script\n",
        "2. Schedule regular runs (e.g., daily)\n",
        "3. Set up alerts for metric thresholds\n",
        "4. Monitor trends over time\n",
        "\n",
        "Example scheduling with cron:\n",
        "```bash\n",
        "0 0 * * * cd /path/to/project && python scripts/run_evaluation.py\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
