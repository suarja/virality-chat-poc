#!/usr/bin/env python3
"""
Script d'analyse rapide pour le POC de virality prediction.
Utilis√© dans la Phase 1 pour valider les donn√©es et cr√©er un mod√®le baseline.
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import argparse
import logging
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import warnings
import joblib
warnings.filterwarnings('ignore')


def setup_logging():
    """Setup logging configuration."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)


def load_and_prepare_data(dataset_dir: str, feature_set: str = 'comprehensive'):
    """
    Charge et pr√©pare les donn√©es pour l'analyse.

    Args:
        dataset_dir: Chemin vers le dossier du dataset
        feature_set: Feature set √† utiliser

    Returns:
        DataFrame pr√©par√© pour l'analyse
    """
    logger = logging.getLogger(__name__)

    # Chercher le fichier agr√©g√© ou agr√©ger si n√©cessaire
    features_dir = Path(dataset_dir) / "features"
    aggregated_file = features_dir / f"aggregated_{feature_set}.csv"

    if aggregated_file.exists():
        logger.info(f"Chargement du fichier agr√©g√©: {aggregated_file}")
        df = pd.read_csv(aggregated_file)
    else:
        logger.info("Fichier agr√©g√© non trouv√©, agr√©gation en cours...")
        from aggregate_features import aggregate_features
        df = aggregate_features(dataset_dir, feature_set)

    logger.info(
        f"Donn√©es charg√©es: {len(df)} vid√©os, {len(df.columns)} features")
    return df


def analyze_data_distribution(df: pd.DataFrame):
    """
    Analyse la distribution des donn√©es et des m√©triques cl√©s.

    Args:
        df: DataFrame avec les features
    """
    logger = logging.getLogger(__name__)

    print("\n" + "="*60)
    print("üìä ANALYSE DE LA DISTRIBUTION DES DONN√âES")
    print("="*60)

    # M√©triques de base
    print(f"\nüìà M√âTRIQUES DE BASE:")
    print(f"   ‚Ä¢ Nombre de vid√©os: {len(df)}")
    print(f"   ‚Ä¢ Nombre de comptes: {df['account_name'].nunique()}")
    print(f"   ‚Ä¢ P√©riode: {df['month'].min()} - {df['month'].max()}")

    # Distribution des vues
    print(f"\nüëÄ DISTRIBUTION DES VUES:")
    view_stats = df['view_count'].describe()
    print(f"   ‚Ä¢ Moyenne: {view_stats['mean']:,.0f}")
    print(f"   ‚Ä¢ M√©diane: {view_stats['50%']:,.0f}")
    print(f"   ‚Ä¢ Min: {view_stats['min']:,.0f}")
    print(f"   ‚Ä¢ Max: {view_stats['max']:,.0f}")
    print(f"   ‚Ä¢ √âcart-type: {view_stats['std']:,.0f}")

    # Distribution par compte
    print(f"\nüì± VUES PAR COMPTE:")
    account_stats = df.groupby('account_name')['view_count'].agg(
        ['count', 'mean', 'median']).round(0)
    for account, stats in account_stats.iterrows():
        print(
            f"   ‚Ä¢ {account}: {stats['count']} vid√©os, {stats['mean']:,.0f} vues moyennes")

    # Engagement
    if 'engagement_rate' in df.columns:
        print(f"\nüí¨ ENGAGEMENT:")
        engagement_stats = df['engagement_rate'].describe()
        print(f"   ‚Ä¢ Taux d'engagement moyen: {engagement_stats['mean']:.3f}")
        print(f"   ‚Ä¢ M√©diane: {engagement_stats['50%']:.3f}")


def analyze_correlations(df: pd.DataFrame, target_col: str = 'view_count'):
    """
    Analyse les corr√©lations avec la variable cible.

    Args:
        df: DataFrame avec les features
        target_col: Colonne cible (par d√©faut 'view_count')
    """
    logger = logging.getLogger(__name__)

    print("\n" + "="*60)
    print("üîó ANALYSE DES CORR√âLATIONS")
    print("="*60)

    # S√©lectionner les colonnes num√©riques
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_cols = [col for col in numeric_cols if col != target_col]

    # Calculer les corr√©lations
    correlations = df[numeric_cols + [target_col]
                      ].corr()[target_col].sort_values(ascending=False)

    print(f"\nüìä TOP 10 CORR√âLATIONS AVEC {target_col.upper()}:")
    for feature, corr in correlations.head(10).items():
        print(f"   ‚Ä¢ {feature}: {corr:.3f}")

    print(f"\nüìâ TOP 10 CORR√âLATIONS N√âGATIVES:")
    for feature, corr in correlations.tail(10).items():
        print(f"   ‚Ä¢ {feature}: {corr:.3f}")

    # Visualisation des corr√©lations
    plt.figure(figsize=(12, 8))
    top_correlations = correlations.head(15)
    plt.barh(range(len(top_correlations)), top_correlations.values)
    plt.yticks(range(len(top_correlations)), top_correlations.index)
    plt.xlabel('Corr√©lation')
    plt.title(f'Top 15 Corr√©lations avec {target_col}')
    plt.tight_layout()
    plt.savefig('correlations_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

    return correlations


def create_baseline_model(df: pd.DataFrame, target_col: str = 'view_count'):
    """
    Cr√©e et √©value un mod√®le baseline.

    Args:
        df: DataFrame avec les features
        target_col: Colonne cible

    Returns:
        Mod√®le entra√Æn√© et m√©triques
    """
    logger = logging.getLogger(__name__)

    print("\n" + "="*60)
    print("ü§ñ MOD√àLE BASELINE")
    print("="*60)

    # Pr√©parer les donn√©es
    feature_cols = [col for col in df.columns if col not in [
        'video_id', target_col, 'account_name']]
    X = df[feature_cols].fillna(0)  # Remplacer les NaN par 0
    y = df[target_col]

    print(f"\nüìã FEATURES UTILIS√âES ({len(feature_cols)}):")
    for i, feature in enumerate(feature_cols, 1):
        print(f"   {i:2d}. {feature}")

    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)

    print(f"\nüìä SPLIT DES DONN√âES:")
    print(f"   ‚Ä¢ Train: {len(X_train)} vid√©os")
    print(f"   ‚Ä¢ Test: {len(X_test)} vid√©os")

    # Mod√®le Random Forest
    model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )

    # Entra√Ænement
    logger.info("Entra√Ænement du mod√®le...")
    model.fit(X_train, y_train)

    # Pr√©dictions
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    # M√©triques
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    train_rmse = mean_squared_error(y_train, y_pred_train, squared=False)
    test_rmse = mean_squared_error(y_test, y_pred_test, squared=False)
    train_mae = mean_absolute_error(y_train, y_pred_train)
    test_mae = mean_absolute_error(y_test, y_pred_test)

    print(f"\nüìà M√âTRIQUES DE PERFORMANCE:")
    print(f"   ‚Ä¢ R¬≤ Score (train): {train_r2:.3f}")
    print(f"   ‚Ä¢ R¬≤ Score (test):  {test_r2:.3f}")
    print(f"   ‚Ä¢ RMSE (train):     {train_rmse:,.0f}")
    print(f"   ‚Ä¢ RMSE (test):      {test_rmse:,.0f}")
    print(f"   ‚Ä¢ MAE (train):      {train_mae:,.0f}")
    print(f"   ‚Ä¢ MAE (test):       {test_mae:,.0f}")

    # Validation crois√©e
    cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
    print(f"\nüîÑ VALIDATION CROIS√âE (5-fold):")
    print(f"   ‚Ä¢ R¬≤ Score moyen: {cv_scores.mean():.3f}")
    print(f"   ‚Ä¢ √âcart-type: {cv_scores.std():.3f}")
    print(
        f"   ‚Ä¢ Intervalle: [{cv_scores.mean() - 2*cv_scores.std():.3f}, {cv_scores.mean() + 2*cv_scores.std():.3f}]")

    # Features importantes
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)

    print(f"\nüèÜ TOP 10 FEATURES IMPORTANTES:")
    for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):
        print(f"   {i:2d}. {row['feature']}: {row['importance']:.3f}")

    # Visualisation des features importantes
    plt.figure(figsize=(12, 8))
    top_features = feature_importance.head(15)
    plt.barh(range(len(top_features)), top_features['importance'])
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Importance')
    plt.title('Top 15 Features Importantes')
    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()

    return model, {
        'train_r2': train_r2,
        'test_r2': test_r2,
        'train_rmse': train_rmse,
        'test_rmse': test_rmse,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'feature_importance': feature_importance
    }


def generate_insights(df: pd.DataFrame, model, feature_importance):
    """
    G√©n√®re des insights bas√©s sur l'analyse.

    Args:
        df: DataFrame avec les features
        model: Mod√®le entra√Æn√©
        feature_importance: DataFrame avec l'importance des features
    """
    logger = logging.getLogger(__name__)

    print("\n" + "="*60)
    print("üí° INSIGHTS ET RECOMMANDATIONS")
    print("="*60)

    # Insights sur les donn√©es
    print(f"\nüìä INSIGHTS SUR LES DONN√âES:")
    print(
        f"   ‚Ä¢ Dataset: {len(df)} vid√©os de {df['account_name'].nunique()} comptes")
    print(f"   ‚Ä¢ P√©riode: {df['month'].min()} - {df['month'].max()}")
    print(f"   ‚Ä¢ Vues moyennes: {df['view_count'].mean():,.0f}")

    # Insights sur les features importantes
    print(f"\nüéØ FEATURES CL√âS POUR LA VIRALIT√â:")
    top_features = feature_importance.head(5)
    for _, row in top_features.iterrows():
        feature = row['feature']
        importance = row['importance']

        if 'engagement' in feature.lower():
            print(
                f"   ‚Ä¢ {feature} ({importance:.3f}): L'engagement est crucial")
        elif 'duration' in feature.lower():
            print(
                f"   ‚Ä¢ {feature} ({importance:.3f}): La dur√©e optimale compte")
        elif 'hashtag' in feature.lower():
            print(
                f"   ‚Ä¢ {feature} ({importance:.3f}): Le nombre de hashtags influence")
        else:
            print(f"   ‚Ä¢ {feature} ({importance:.3f}): Feature importante")

    # Recommandations
    print(f"\nüöÄ RECOMMANDATIONS POUR LA SUITE:")
    print(
        f"   ‚Ä¢ Mod√®le baseline: R¬≤ = {model.cv_scores.mean():.3f} (acceptable pour POC)")
    print(f"   ‚Ä¢ Prochaine √©tape: Augmenter le dataset √† 150+ vid√©os")
    print(f"   ‚Ä¢ Optimisation: Tester Gradient Boosting et XGBoost")
    print(f"   ‚Ä¢ Features: Ajouter plus de features temporelles et de contenu")


def main():
    """Main function."""
    parser = argparse.ArgumentParser(
        description="Analyse rapide des donn√©es pour le POC de virality prediction"
    )

    parser.add_argument(
        "--dataset-dir",
        type=str,
        required=True,
        help="Chemin vers le dossier du dataset (ex: data/dataset_poc_validation)"
    )

    parser.add_argument(
        "--feature-set",
        type=str,
        default='comprehensive',
        choices=['metadata', 'gemini_basic',
                 'visual_granular', 'comprehensive'],
        help="Feature set √† analyser"
    )

    parser.add_argument(
        "--target",
        type=str,
        default='view_count',
        help="Variable cible pour la pr√©diction"
    )

    parser.add_argument(
        "--save-model",
        action="store_true",
        help="Sauvegarder le mod√®le entra√Æn√©"
    )

    args = parser.parse_args()
    logger = setup_logging()

    try:
        # Charger les donn√©es
        logger.info(f"Chargement des donn√©es depuis {args.dataset_dir}")
        df = load_and_prepare_data(args.dataset_dir, args.feature_set)

        # Analyser la distribution
        analyze_data_distribution(df)

        # Analyser les corr√©lations
        correlations = analyze_correlations(df, args.target)

        # Cr√©er le mod√®le baseline
        model, metrics = create_baseline_model(df, args.target)

        # G√©n√©rer les insights
        generate_insights(df, model, metrics['feature_importance'])

        # Sauvegarder le mod√®le si demand√©
        if args.save_model:
            model_path = Path('models')
            model_path.mkdir(exist_ok=True)

            joblib.dump(model, model_path / 'baseline_virality_model.pkl')
            logger.info(
                f"Mod√®le sauvegard√© dans {model_path / 'baseline_virality_model.pkl'}")

        # R√©sum√© final
        print("\n" + "="*60)
        print("‚úÖ ANALYSE TERMIN√âE")
        print("="*60)
        print(f"üìä Performance du mod√®le: R¬≤ = {metrics['test_r2']:.3f}")
        print(f"üéØ Prochaine √©tape: Augmenter le dataset et optimiser le mod√®le")
        print(f"üìÅ Visualisations sauvegard√©es: correlations_analysis.png, feature_importance.png")

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'analyse: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
