#!/usr/bin/env python3
"""
Script pour analyser les donn√©es existantes du POC.
Utilis√© quand le pipeline a d√©j√† g√©n√©r√© les features mais qu'il manque l'agr√©gation.
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import argparse
import logging
import sys
import warnings
warnings.filterwarnings('ignore')


def setup_logging():
    """Setup logging configuration."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)


def aggregate_existing_features(dataset_dir: str, feature_set: str = 'comprehensive'):
    """
    Agr√®ge les features existantes par compte en un seul fichier.

    Args:
        dataset_dir: Chemin vers le dossier du dataset
        feature_set: Feature set √† utiliser

    Returns:
        DataFrame agr√©g√©
    """
    logger = logging.getLogger(__name__)

    features_dir = Path(dataset_dir) / "features"
    aggregated_file = features_dir / f"aggregated_{feature_set}.csv"

    # V√©rifier si l'agr√©gation existe d√©j√†
    if aggregated_file.exists():
        logger.info(f"Fichier agr√©g√© existant trouv√©: {aggregated_file}")
        return pd.read_csv(aggregated_file)

    # Chercher les fichiers de features par compte
    feature_files = list(features_dir.glob(f"*_features_{feature_set}.csv"))

    # Si pas de fichiers avec le pattern sp√©cifique, chercher des fichiers CSV g√©n√©riques
    if not feature_files:
        feature_files = list(features_dir.glob("*.csv"))

    if not feature_files:
        raise FileNotFoundError(
            f"Aucun fichier de features trouv√© dans {features_dir}")

    logger.info(f"Agr√©gation de {len(feature_files)} fichiers de features...")

    # Charger et concat√©ner les features
    all_features = []
    for file_path in feature_files:
        account_name = file_path.stem.replace(f'_features_{feature_set}', '')
        logger.info(f"Chargement de {account_name}: {file_path}")

        df = pd.read_csv(file_path)
        df['account_name'] = account_name
        all_features.append(df)

    # Concat√©ner tous les DataFrames
    aggregated_df = pd.concat(all_features, ignore_index=True)

    # Sauvegarder l'agr√©gation
    aggregated_df.to_csv(aggregated_file, index=False)
    logger.info(f"‚úÖ Features agr√©g√©es sauvegard√©es dans {aggregated_file}")

    return aggregated_df


def analyze_data_distribution(df: pd.DataFrame):
    """Analyse la distribution des donn√©es."""
    logger = logging.getLogger(__name__)

    print("\n" + "="*60)
    print("üìä ANALYSE DE LA DISTRIBUTION DES DONN√âES")
    print("="*60)

    # M√©triques de base
    print(f"\nüìà M√âTRIQUES DE BASE:")
    print(f"   ‚Ä¢ Nombre de vid√©os: {len(df)}")
    print(f"   ‚Ä¢ Nombre de comptes: {df['account_name'].nunique()}")
    print(f"   ‚Ä¢ Comptes: {', '.join(df['account_name'].unique())}")

    # Distribution des vues
    if 'view_count' in df.columns:
        print(f"\nüëÄ DISTRIBUTION DES VUES:")
        view_stats = df['view_count'].describe()
        print(f"   ‚Ä¢ Moyenne: {view_stats['mean']:,.0f}")
        print(f"   ‚Ä¢ M√©diane: {view_stats['50%']:,.0f}")
        print(f"   ‚Ä¢ Min: {view_stats['min']:,.0f}")
        print(f"   ‚Ä¢ Max: {view_stats['max']:,.0f}")
        print(f"   ‚Ä¢ √âcart-type: {view_stats['std']:,.0f}")

        # Distribution par compte
        print(f"\nüì± VUES PAR COMPTE:")
        account_stats = df.groupby('account_name')['view_count'].agg(
            ['count', 'mean', 'median']).round(0)
        for account, stats in account_stats.iterrows():
            print(
                f"   ‚Ä¢ {account}: {stats['count']} vid√©os, {stats['mean']:,.0f} vues moyennes")

    # Features disponibles
    feature_cols = [col for col in df.columns if col not in [
        'video_id', 'view_count', 'account_name']]
    print(f"\nüîß FEATURES DISPONIBLES ({len(feature_cols)}):")
    for i, feature in enumerate(feature_cols[:15], 1):
        print(f"   {i:2d}. {feature}")

    if len(feature_cols) > 15:
        print(f"   ... et {len(feature_cols) - 15} autres")


def analyze_correlations(df: pd.DataFrame, target_col: str = 'view_count'):
    """Analyse les corr√©lations avec la variable cible."""
    logger = logging.getLogger(__name__)

    if target_col not in df.columns:
        logger.warning(f"Colonne cible '{target_col}' non trouv√©e")
        return None

    print("\n" + "="*60)
    print("üîó ANALYSE DES CORR√âLATIONS")
    print("="*60)

    # S√©lectionner les colonnes num√©riques
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_cols = [col for col in numeric_cols if col != target_col]

    if not numeric_cols:
        logger.warning(
            "Aucune colonne num√©rique trouv√©e pour l'analyse des corr√©lations")
        return None

    # Calculer les corr√©lations
    correlations = df[numeric_cols + [target_col]
                      ].corr()[target_col].sort_values(ascending=False)

    print(f"\nüìä TOP 10 CORR√âLATIONS AVEC {target_col.upper()}:")
    for feature, corr in correlations.head(10).items():
        print(f"   ‚Ä¢ {feature}: {corr:.3f}")

    print(f"\nüìâ TOP 5 CORR√âLATIONS N√âGATIVES:")
    for feature, corr in correlations.tail(5).items():
        print(f"   ‚Ä¢ {feature}: {corr:.3f}")

    return correlations


def create_baseline_model(df: pd.DataFrame, target_col: str = 'view_count'):
    """Cr√©e et √©value un mod√®le baseline."""
    logger = logging.getLogger(__name__)

    if target_col not in df.columns:
        logger.error(f"Colonne cible '{target_col}' non trouv√©e")
        return None, None

    print("\n" + "="*60)
    print("ü§ñ MOD√àLE BASELINE")
    print("="*60)

    try:
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.model_selection import train_test_split, cross_val_score
        from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
    except ImportError as e:
        logger.error(f"D√©pendances manquantes: {e}")
        logger.info("Installer: pip install scikit-learn")
        return None, None

    # Pr√©parer les donn√©es - exclure les colonnes non num√©riques
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    feature_cols = [col for col in numeric_cols if col not in [
        'video_id', target_col, 'account_name']]

    if not feature_cols:
        logger.error("Aucune feature num√©rique trouv√©e pour le mod√®le")
        return None, None

    X = df[feature_cols].fillna(0)  # Remplacer les NaN par 0
    y = df[target_col]

    print(f"\nüìã FEATURES UTILIS√âES ({len(feature_cols)}):")
    for i, feature in enumerate(feature_cols[:10], 1):
        print(f"   {i:2d}. {feature}")

    if len(feature_cols) > 10:
        print(f"   ... et {len(feature_cols) - 10} autres")

    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)

    print(f"\nüìä SPLIT DES DONN√âES:")
    print(f"   ‚Ä¢ Train: {len(X_train)} vid√©os")
    print(f"   ‚Ä¢ Test: {len(X_test)} vid√©os")

    # Mod√®le Random Forest
    model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )

    # Entra√Ænement
    logger.info("Entra√Ænement du mod√®le...")
    model.fit(X_train, y_train)

    # Pr√©dictions
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    # M√©triques
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

    print(f"\nüìà M√âTRIQUES DE PERFORMANCE:")
    print(f"   ‚Ä¢ R¬≤ Score (train): {train_r2:.3f}")
    print(f"   ‚Ä¢ R¬≤ Score (test):  {test_r2:.3f}")
    print(f"   ‚Ä¢ RMSE (train):     {train_rmse:,.0f}")
    print(f"   ‚Ä¢ RMSE (test):      {test_rmse:,.0f}")

    # Validation crois√©e
    cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
    print(f"\nüîÑ VALIDATION CROIS√âE (5-fold):")
    print(f"   ‚Ä¢ R¬≤ Score moyen: {cv_scores.mean():.3f}")
    print(f"   ‚Ä¢ √âcart-type: {cv_scores.std():.3f}")

    # Features importantes
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)

    print(f"\nüèÜ TOP 10 FEATURES IMPORTANTES:")
    for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):
        print(f"   {i:2d}. {row['feature']}: {row['importance']:.3f}")

    return model, {
        'train_r2': train_r2,
        'test_r2': test_r2,
        'train_rmse': train_rmse,
        'test_rmse': test_rmse,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'feature_importance': feature_importance
    }


def generate_insights(df: pd.DataFrame, model, metrics):
    """G√©n√®re des insights bas√©s sur l'analyse."""
    logger = logging.getLogger(__name__)

    print("\n" + "="*60)
    print("üí° INSIGHTS ET RECOMMANDATIONS")
    print("="*60)

    # Insights sur les donn√©es
    print(f"\nüìä INSIGHTS SUR LES DONN√âES:")
    print(
        f"   ‚Ä¢ Dataset: {len(df)} vid√©os de {df['account_name'].nunique()} comptes")
    print(f"   ‚Ä¢ Comptes analys√©s: {', '.join(df['account_name'].unique())}")

    if 'view_count' in df.columns:
        print(f"   ‚Ä¢ Vues moyennes: {df['view_count'].mean():,.0f}")

    # Insights sur le mod√®le
    if model is not None and isinstance(metrics, dict):
        print(f"\nü§ñ INSIGHTS SUR LE MOD√àLE:")
        print(f"   ‚Ä¢ Performance: R¬≤ = {metrics['test_r2']:.3f}")

        if metrics['test_r2'] > 0.6:
            print(f"   ‚Ä¢ √âvaluation: Excellent pour un POC")
        elif metrics['test_r2'] > 0.4:
            print(f"   ‚Ä¢ √âvaluation: Bon pour un POC")
        elif metrics['test_r2'] > 0.2:
            print(f"   ‚Ä¢ √âvaluation: Acceptable pour un POC")
        else:
            print(f"   ‚Ä¢ √âvaluation: √Ä am√©liorer")

    # Recommandations
    print(f"\nüöÄ RECOMMANDATIONS POUR LA SUITE:")
    print(f"   ‚Ä¢ Prochaine √©tape: Augmenter le dataset √† 150+ vid√©os")
    print(f"   ‚Ä¢ Optimisation: Tester Gradient Boosting et XGBoost")
    print(f"   ‚Ä¢ Features: Ajouter plus de features temporelles et de contenu")
    print(f"   ‚Ä¢ Validation: Tester sur de nouveaux comptes")


def main():
    """Main function."""
    parser = argparse.ArgumentParser(
        description="Analyse des donn√©es existantes pour le POC de virality prediction"
    )

    parser.add_argument(
        "--dataset-dir",
        type=str,
        required=True,
        help="Chemin vers le dossier du dataset (ex: data/dataset_poc_test)"
    )

    parser.add_argument(
        "--feature-set",
        type=str,
        default='comprehensive',
        choices=['metadata', 'gemini_basic',
                 'visual_granular', 'comprehensive'],
        help="Feature set √† analyser"
    )

    parser.add_argument(
        "--target",
        type=str,
        default='view_count',
        help="Variable cible pour la pr√©diction"
    )

    parser.add_argument(
        "--save-model",
        action="store_true",
        help="Sauvegarder le mod√®le entra√Æn√©"
    )

    args = parser.parse_args()
    logger = setup_logging()

    try:
        # Agr√©gation des features existantes
        logger.info(f"Analyse des donn√©es depuis {args.dataset_dir}")
        df = aggregate_existing_features(args.dataset_dir, args.feature_set)

        # Analyser la distribution
        analyze_data_distribution(df)

        # Analyser les corr√©lations
        correlations = analyze_correlations(df, args.target)

        # Cr√©er le mod√®le baseline
        model, metrics = create_baseline_model(df, args.target)

        # G√©n√©rer les insights
        if model and metrics:
            generate_insights(df, model, metrics)
        else:
            generate_insights(df, None, None)

        # Sauvegarder le mod√®le si demand√©
        if args.save_model and model:
            import joblib
            model_path = Path('models')
            model_path.mkdir(exist_ok=True)

            joblib.dump(model, model_path / 'baseline_virality_model.pkl')
            logger.info(
                f"Mod√®le sauvegard√© dans {model_path / 'baseline_virality_model.pkl'}")

        # R√©sum√© final
        print("\n" + "="*60)
        print("‚úÖ ANALYSE TERMIN√âE")
        print("="*60)
        if metrics:
            print(f"üìä Performance du mod√®le: R¬≤ = {metrics['test_r2']:.3f}")
        print(f"üéØ Prochaine √©tape: Augmenter le dataset et optimiser le mod√®le")

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'analyse: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
