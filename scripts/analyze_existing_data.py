#!/usr/bin/env python3
"""
Script pour analyser les donnÃ©es existantes du POC.
UtilisÃ© quand le pipeline a dÃ©jÃ  gÃ©nÃ©rÃ© les features mais qu'il manque l'agrÃ©gation.
CORRIGÃ‰ : ImplÃ©mente la logique correcte de sÃ©paration prÃ©-publication/post-publication.
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import argparse
import logging
import sys
import warnings
warnings.filterwarnings('ignore')


def setup_logging():
    """Setup logging configuration."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)


def categorize_features(df: pd.DataFrame):
    """
    CatÃ©gorise les features en prÃ©-publication et post-publication.

    Args:
        df: DataFrame avec toutes les features

    Returns:
        dict: Dictionnaire avec features prÃ©-publication et post-publication
    """
    logger = logging.getLogger(__name__)

    # Features PRÃ‰-PUBLICATION (disponibles avant publication)
    pre_publication_features = [
        # MÃ©tadonnÃ©es vidÃ©o
        'duration', 'hashtag_count', 'estimated_hashtag_count',

        # Features temporelles
        'hour_of_day', 'day_of_week', 'month', 'is_weekend', 'is_business_hours',

        # Features Gemini (analyse visuelle)
        'has_text_overlays', 'has_transitions', 'visual_quality_score',
        'has_hook', 'has_story', 'has_call_to_action', 'viral_potential_score',
        'emotional_trigger_count', 'audience_connection_score', 'length_optimized',
        'sound_quality_score', 'production_quality_score', 'trend_alignment_score',

        # Features visuelles granulaires
        'close_up_presence', 'zoom_effects_count', 'transition_count',
        'color_vibrancy_score', 'human_presence', 'face_count',
        'text_overlay_presence', 'camera_movement_score', 'brightness_score',
        'contrast_score', 'video_duration_optimized',

        # Features transformÃ©es de dates
        'post_time_hour', 'post_time_day_of_week', 'post_time_month',
        'post_time_is_weekend', 'post_time_is_business_hours',
        'extraction_time_hour', 'extraction_time_day_of_week', 'extraction_time_month',
        'extraction_time_is_weekend', 'extraction_time_is_business_hours'
    ]

    # Features POST-PUBLICATION (disponibles seulement aprÃ¨s publication)
    post_publication_features = [
        'view_count', 'like_count', 'comment_count', 'share_count',
        'like_rate', 'comment_rate', 'share_rate', 'engagement_rate'
    ]

    # Filtrer les features qui existent rÃ©ellement dans le dataset
    available_features = df.columns.tolist()

    pre_pub_available = [
        f for f in pre_publication_features if f in available_features]
    post_pub_available = [
        f for f in post_publication_features if f in available_features]

    logger.info(f"Features prÃ©-publication trouvÃ©es: {len(pre_pub_available)}")
    logger.info(
        f"Features post-publication trouvÃ©es: {len(post_pub_available)}")

    return {
        'pre_publication': pre_pub_available,
        'post_publication': post_pub_available,
        'all_features': available_features
    }


def aggregate_existing_features(dataset_dir: str, feature_set: str = 'comprehensive'):
    """
    AgrÃ¨ge les features existantes par compte en un seul fichier.

    Args:
        dataset_dir: Chemin vers le dossier du dataset
        feature_set: Feature set Ã  utiliser

    Returns:
        DataFrame agrÃ©gÃ©
    """
    logger = logging.getLogger(__name__)

    features_dir = Path(dataset_dir) / "features"
    aggregated_file = features_dir / f"aggregated_{feature_set}.csv"

    # VÃ©rifier si l'agrÃ©gation existe dÃ©jÃ 
    if aggregated_file.exists():
        logger.info(f"Fichier agrÃ©gÃ© existant trouvÃ©: {aggregated_file}")
        return pd.read_csv(aggregated_file)

    # Chercher les fichiers de features par compte
    feature_files = list(features_dir.glob(f"*_features_{feature_set}.csv"))

    # Si pas de fichiers avec le pattern spÃ©cifique, chercher des fichiers CSV gÃ©nÃ©riques
    if not feature_files:
        feature_files = list(features_dir.glob("*.csv"))

    if not feature_files:
        raise FileNotFoundError(
            f"Aucun fichier de features trouvÃ© dans {features_dir}")

    logger.info(f"AgrÃ©gation de {len(feature_files)} fichiers de features...")

    # Charger et concatÃ©ner les features
    all_features = []
    for file_path in feature_files:
        account_name = file_path.stem.replace(f'_features_{feature_set}', '')
        logger.info(f"Chargement de {account_name}: {file_path}")

        df = pd.read_csv(file_path)
        df['account_name'] = account_name
        all_features.append(df)

    # ConcatÃ©ner tous les DataFrames
    aggregated_df = pd.concat(all_features, ignore_index=True)

    # Sauvegarder l'agrÃ©gation
    aggregated_df.to_csv(aggregated_file, index=False)
    logger.info(f"âœ… Features agrÃ©gÃ©es sauvegardÃ©es dans {aggregated_file}")

    return aggregated_df


def analyze_data_distribution(df: pd.DataFrame):
    """Analyse la distribution des donnÃ©es."""
    logger = logging.getLogger(__name__)

    print("\n" + "="*60)
    print("ðŸ“Š ANALYSE DE LA DISTRIBUTION DES DONNÃ‰ES")
    print("="*60)

    # MÃ©triques de base
    print(f"\nðŸ“ˆ MÃ‰TRIQUES DE BASE:")
    print(f"   â€¢ Nombre de vidÃ©os: {len(df)}")
    print(f"   â€¢ Nombre de comptes: {df['account_name'].nunique()}")
    print(f"   â€¢ Comptes: {', '.join(df['account_name'].unique())}")

    # Distribution des vues
    if 'view_count' in df.columns:
        print(f"\nðŸ‘€ DISTRIBUTION DES VUES:")
        view_stats = df['view_count'].describe()
        print(f"   â€¢ Moyenne: {view_stats['mean']:,.0f}")
        print(f"   â€¢ MÃ©diane: {view_stats['50%']:,.0f}")
        print(f"   â€¢ Min: {view_stats['min']:,.0f}")
        print(f"   â€¢ Max: {view_stats['max']:,.0f}")
        print(f"   â€¢ Ã‰cart-type: {view_stats['std']:,.0f}")

        # Distribution par compte
        print(f"\nðŸ“± VUES PAR COMPTE:")
        account_stats = df.groupby('account_name')['view_count'].agg(
            ['count', 'mean', 'median']).round(0)
        for account, stats in account_stats.iterrows():
            print(
                f"   â€¢ {account}: {stats['count']} vidÃ©os, {stats['mean']:,.0f} vues moyennes")

    # Features disponibles
    feature_cols = [col for col in df.columns if col not in [
        'video_id', 'view_count', 'account_name']]
    print(f"\nðŸ”§ FEATURES DISPONIBLES ({len(feature_cols)}):")
    for i, feature in enumerate(feature_cols[:15], 1):
        print(f"   {i:2d}. {feature}")

    if len(feature_cols) > 15:
        print(f"   ... et {len(feature_cols) - 15} autres")


def analyze_correlations(df: pd.DataFrame, target_col: str = 'view_count'):
    """Analyse les corrÃ©lations avec la variable cible."""
    logger = logging.getLogger(__name__)

    if target_col not in df.columns:
        logger.warning(f"Colonne cible '{target_col}' non trouvÃ©e")
        return None

    print("\n" + "="*60)
    print("ðŸ”— ANALYSE DES CORRÃ‰LATIONS")
    print("="*60)

    # SÃ©lectionner les colonnes numÃ©riques
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_cols = [col for col in numeric_cols if col != target_col]

    if not numeric_cols:
        logger.warning(
            "Aucune colonne numÃ©rique trouvÃ©e pour l'analyse des corrÃ©lations")
        return None

    # Calculer les corrÃ©lations
    correlations = df[numeric_cols + [target_col]
                      ].corr()[target_col].sort_values(ascending=False)

    print(f"\nðŸ“Š TOP 10 CORRÃ‰LATIONS AVEC {target_col.upper()}:")
    for feature, corr in correlations.head(10).items():
        print(f"   â€¢ {feature}: {corr:.3f}")

    print(f"\nðŸ“‰ TOP 5 CORRÃ‰LATIONS NÃ‰GATIVES:")
    for feature, corr in correlations.tail(5).items():
        print(f"   â€¢ {feature}: {corr:.3f}")

    return correlations


def create_baseline_model(df: pd.DataFrame, target_col: str = 'view_count'):
    """CrÃ©e et Ã©value un modÃ¨le baseline avec sÃ©paration prÃ©-publication/post-publication."""
    logger = logging.getLogger(__name__)

    if target_col not in df.columns:
        logger.error(f"Colonne cible '{target_col}' non trouvÃ©e")
        return None, None

    print("\n" + "="*60)
    print("ðŸ¤– MODÃˆLE BASELINE - LOGIQUE CORRIGÃ‰E")
    print("="*60)

    try:
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.model_selection import train_test_split, cross_val_score
        from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
    except ImportError as e:
        logger.error(f"DÃ©pendances manquantes: {e}")
        logger.info("Installer: pip install scikit-learn")
        return None, None

    # PrÃ©parer les donnÃ©es - transformer les dates et exclure les colonnes non numÃ©riques
    df_processed = df.copy()

    # Transformer les colonnes de dates en features numÃ©riques
    date_columns = ['post_time', 'extraction_time']
    for col in date_columns:
        if col in df_processed.columns:
            try:
                # Convertir en datetime
                df_processed[col] = pd.to_datetime(df_processed[col])

                # Extraire les features temporelles
                df_processed[f'{col}_hour'] = df_processed[col].dt.hour
                df_processed[f'{col}_day_of_week'] = df_processed[col].dt.dayofweek
                df_processed[f'{col}_month'] = df_processed[col].dt.month
                df_processed[f'{col}_is_weekend'] = df_processed[col].dt.dayofweek.isin([
                                                                                        5, 6]).astype(int)
                df_processed[f'{col}_is_business_hours'] = ((df_processed[col].dt.hour >= 9) &
                                                            (df_processed[col].dt.hour <= 17)).astype(int)

                # Supprimer la colonne originale
                df_processed = df_processed.drop(columns=[col])

            except Exception as e:
                logger.warning(f"Impossible de traiter la colonne {col}: {e}")
                df_processed = df_processed.drop(columns=[col])

    # CatÃ©goriser les features
    feature_categories = categorize_features(df_processed)

    print(f"\nðŸ“‹ CATÃ‰GORISATION DES FEATURES:")
    print(
        f"   â€¢ PrÃ©-publication: {len(feature_categories['pre_publication'])} features")
    print(
        f"   â€¢ Post-publication: {len(feature_categories['post_publication'])} features")

    # Exclure les colonnes non numÃ©riques
    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns
    all_feature_cols = [col for col in numeric_cols if col not in [
        'video_id', target_col, 'account_name']]

    # Filtrer les features par catÃ©gorie
    pre_pub_features = [
        f for f in feature_categories['pre_publication'] if f in all_feature_cols]
    post_pub_features = [
        f for f in feature_categories['post_publication'] if f in all_feature_cols]

    print(
        f"\nðŸ” FEATURES PRÃ‰-PUBLICATION DISPONIBLES ({len(pre_pub_features)}):")
    for i, feature in enumerate(pre_pub_features[:10], 1):
        print(f"   {i:2d}. {feature}")
    if len(pre_pub_features) > 10:
        print(f"   ... et {len(pre_pub_features) - 10} autres")

    print(
        f"\nðŸ“Š FEATURES POST-PUBLICATION DISPONIBLES ({len(post_pub_features)}):")
    for i, feature in enumerate(post_pub_features, 1):
        print(f"   {i:2d}. {feature}")

    # PHASE 1: EntraÃ®nement avec toutes les features (pour apprendre les patterns)
    print(f"\nðŸŽ¯ PHASE 1: ENTRAÃŽNEMENT AVEC FEATURES COMPLÃˆTES")
    X_full = df_processed[all_feature_cols].fillna(0)
    y = df_processed[target_col]

    # Split train/test
    X_train_full, X_test_full, y_train, y_test = train_test_split(
        X_full, y, test_size=0.2, random_state=42)

    print(f"   â€¢ Train: {len(X_train_full)} vidÃ©os")
    print(f"   â€¢ Test: {len(X_test_full)} vidÃ©os")

    # ModÃ¨le avec toutes les features
    model_full = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )

    # EntraÃ®nement
    logger.info("EntraÃ®nement du modÃ¨le avec features complÃ¨tes...")
    model_full.fit(X_train_full, y_train)

    # PrÃ©dictions avec features complÃ¨tes
    y_pred_train_full = model_full.predict(X_train_full)
    y_pred_test_full = model_full.predict(X_test_full)

    # MÃ©triques avec features complÃ¨tes
    train_r2_full = r2_score(y_train, y_pred_train_full)
    test_r2_full = r2_score(y_test, y_pred_test_full)

    print(f"   â€¢ RÂ² Score (train): {train_r2_full:.3f}")
    print(f"   â€¢ RÂ² Score (test):  {test_r2_full:.3f}")

    # PHASE 2: Test avec features prÃ©-publication seulement
    print(f"\nðŸŽ¯ PHASE 2: TEST AVEC FEATURES PRÃ‰-PUBLICATION SEULEMENT")

    if not pre_pub_features:
        logger.error("Aucune feature prÃ©-publication disponible pour le test")
        return None, None

    # SÃ©lectionner seulement les features prÃ©-publication
    X_pre_pub = df_processed[pre_pub_features].fillna(0)

    # Split train/test avec features prÃ©-publication
    X_train_pre, X_test_pre, y_train_pre, y_test_pre = train_test_split(
        X_pre_pub, y, test_size=0.2, random_state=42)

    # ModÃ¨le avec features prÃ©-publication seulement
    model_pre_pub = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )

    # EntraÃ®nement avec features prÃ©-publication
    logger.info("EntraÃ®nement du modÃ¨le avec features prÃ©-publication...")
    model_pre_pub.fit(X_train_pre, y_train_pre)

    # PrÃ©dictions avec features prÃ©-publication
    y_pred_train_pre = model_pre_pub.predict(X_train_pre)
    y_pred_test_pre = model_pre_pub.predict(X_test_pre)

    # MÃ©triques avec features prÃ©-publication
    train_r2_pre = r2_score(y_train_pre, y_pred_train_pre)
    test_r2_pre = r2_score(y_test_pre, y_pred_test_pre)
    train_rmse_pre = np.sqrt(mean_squared_error(y_train_pre, y_pred_train_pre))
    test_rmse_pre = np.sqrt(mean_squared_error(y_test_pre, y_pred_test_pre))

    print(f"   â€¢ RÂ² Score (train): {train_r2_pre:.3f}")
    print(f"   â€¢ RÂ² Score (test):  {test_r2_pre:.3f}")
    print(f"   â€¢ RMSE (train):     {train_rmse_pre:,.0f}")
    print(f"   â€¢ RMSE (test):      {test_rmse_pre:,.0f}")

    # Validation croisÃ©e avec features prÃ©-publication
    cv_scores_pre = cross_val_score(
        model_pre_pub, X_pre_pub, y, cv=5, scoring='r2')
    print(f"\nðŸ”„ VALIDATION CROISÃ‰E (5-fold) - Features prÃ©-publication:")
    print(f"   â€¢ RÂ² Score moyen: {cv_scores_pre.mean():.3f}")
    print(f"   â€¢ Ã‰cart-type: {cv_scores_pre.std():.3f}")

    # Features importantes (prÃ©-publication)
    feature_importance_pre = pd.DataFrame({
        'feature': pre_pub_features,
        'importance': model_pre_pub.feature_importances_
    }).sort_values('importance', ascending=False)

    print(f"\nðŸ† TOP 10 FEATURES IMPORTANTES (PRÃ‰-PUBLICATION):")
    for i, (_, row) in enumerate(feature_importance_pre.head(10).iterrows(), 1):
        print(f"   {i:2d}. {row['feature']}: {row['importance']:.3f}")

    # Comparaison des performances
    print(f"\nðŸ“Š COMPARAISON DES PERFORMANCES:")
    print(f"   â€¢ Avec features complÃ¨tes: RÂ² = {test_r2_full:.3f}")
    print(f"   â€¢ Avec features prÃ©-publication: RÂ² = {test_r2_pre:.3f}")
    print(
        f"   â€¢ Perte de performance: {((test_r2_full - test_r2_pre) / test_r2_full * 100):.1f}%")

    return model_pre_pub, {
        'train_r2': train_r2_pre,
        'test_r2': test_r2_pre,
        'train_rmse': train_rmse_pre,
        'test_rmse': test_rmse_pre,
        'cv_mean': cv_scores_pre.mean(),
        'cv_std': cv_scores_pre.std(),
        'feature_importance': feature_importance_pre,
        'pre_publication_features': pre_pub_features,
        'performance_comparison': {
            'full_features_r2': test_r2_full,
            'pre_publication_r2': test_r2_pre,
            'performance_loss': ((test_r2_full - test_r2_pre) / test_r2_full * 100)
        }
    }


def generate_insights(df: pd.DataFrame, model, metrics):
    """GÃ©nÃ¨re des insights basÃ©s sur l'analyse."""
    logger = logging.getLogger(__name__)

    print("\n" + "="*60)
    print("ðŸ’¡ INSIGHTS ET RECOMMANDATIONS")
    print("="*60)

    # Insights sur les donnÃ©es
    print(f"\nðŸ“Š INSIGHTS SUR LES DONNÃ‰ES:")
    print(
        f"   â€¢ Dataset: {len(df)} vidÃ©os de {df['account_name'].nunique()} comptes")
    print(f"   â€¢ Comptes analysÃ©s: {', '.join(df['account_name'].unique())}")

    if 'view_count' in df.columns:
        print(f"   â€¢ Vues moyennes: {df['view_count'].mean():,.0f}")

    # Insights sur le modÃ¨le
    if model is not None and isinstance(metrics, dict):
        print(f"\nðŸ¤– INSIGHTS SUR LE MODÃˆLE PRÃ‰-PUBLICATION:")
        print(f"   â€¢ Performance: RÂ² = {metrics['test_r2']:.3f}")

        if metrics['test_r2'] > 0.6:
            print(f"   â€¢ Ã‰valuation: Excellent pour un POC")
        elif metrics['test_r2'] > 0.4:
            print(f"   â€¢ Ã‰valuation: Bon pour un POC")
        elif metrics['test_r2'] > 0.2:
            print(f"   â€¢ Ã‰valuation: Acceptable pour un POC")
        else:
            print(f"   â€¢ Ã‰valuation: Ã€ amÃ©liorer")

        # Comparaison des performances
        if 'performance_comparison' in metrics:
            comp = metrics['performance_comparison']
            print(f"\nðŸ“ˆ COMPARAISON AVEC FEATURES COMPLÃˆTES:")
            print(
                f"   â€¢ Features complÃ¨tes: RÂ² = {comp['full_features_r2']:.3f}")
            print(
                f"   â€¢ Features prÃ©-publication: RÂ² = {comp['pre_publication_r2']:.3f}")
            print(
                f"   â€¢ Perte de performance: {comp['performance_loss']:.1f}%")

            if comp['performance_loss'] < 20:
                print(f"   â€¢ âœ… Bonne performance avec features prÃ©-publication")
            elif comp['performance_loss'] < 40:
                print(f"   â€¢ âš ï¸ Performance acceptable, Ã  amÃ©liorer")
            else:
                print(f"   â€¢ âŒ Performance insuffisante, besoin de plus de features")

    # Recommandations
    print(f"\nðŸš€ RECOMMANDATIONS POUR LA SUITE:")
    print(f"   â€¢ Prochaine Ã©tape: Augmenter le dataset Ã  150+ vidÃ©os")
    print(f"   â€¢ Optimisation: Tester Gradient Boosting et XGBoost")
    print(f"   â€¢ Features: Ajouter plus de features visuelles et temporelles")
    print(f"   â€¢ Validation: Tester sur de nouveaux comptes")
    print(f"   â€¢ API: CrÃ©er l'API de prÃ©diction avec features prÃ©-publication")


def main():
    """Main function."""
    parser = argparse.ArgumentParser(
        description="Analyse des donnÃ©es existantes pour le POC de virality prediction (CORRIGÃ‰)"
    )

    parser.add_argument(
        "--dataset-dir",
        type=str,
        required=True,
        help="Chemin vers le dossier du dataset (ex: data/dataset_poc_test)"
    )

    parser.add_argument(
        "--feature-set",
        type=str,
        default='comprehensive',
        choices=['metadata', 'gemini_basic',
                 'visual_granular', 'comprehensive'],
        help="Feature set Ã  analyser"
    )

    parser.add_argument(
        "--target",
        type=str,
        default='view_count',
        help="Variable cible pour la prÃ©diction"
    )

    parser.add_argument(
        "--save-model",
        action="store_true",
        help="Sauvegarder le modÃ¨le entraÃ®nÃ©"
    )

    args = parser.parse_args()
    logger = setup_logging()

    try:
        # AgrÃ©gation des features existantes
        logger.info(f"Analyse des donnÃ©es depuis {args.dataset_dir}")
        df = aggregate_existing_features(args.dataset_dir, args.feature_set)

        # Analyser la distribution
        analyze_data_distribution(df)

        # Analyser les corrÃ©lations
        correlations = analyze_correlations(df, args.target)

        # CrÃ©er le modÃ¨le baseline (CORRIGÃ‰)
        model, metrics = create_baseline_model(df, args.target)

        # GÃ©nÃ©rer les insights
        if model and metrics:
            generate_insights(df, model, metrics)
        else:
            generate_insights(df, None, None)

        # Sauvegarder le modÃ¨le si demandÃ©
        if args.save_model and model:
            import joblib
            model_path = Path('models')
            model_path.mkdir(exist_ok=True)

            joblib.dump(model, model_path /
                        'pre_publication_virality_model.pkl')
            logger.info(
                f"ModÃ¨le prÃ©-publication sauvegardÃ© dans {model_path / 'pre_publication_virality_model.pkl'}")

        # RÃ©sumÃ© final
        print("\n" + "="*60)
        print("âœ… ANALYSE TERMINÃ‰E - LOGIQUE CORRIGÃ‰E")
        print("="*60)
        if metrics:
            print(
                f"ðŸ“Š Performance du modÃ¨le prÃ©-publication: RÂ² = {metrics['test_r2']:.3f}")
        print(f"ðŸŽ¯ Prochaine Ã©tape: Augmenter le dataset et optimiser le modÃ¨le")

    except Exception as e:
        logger.error(f"âŒ Erreur lors de l'analyse: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
