#!/usr/bin/env python3
"""
Script de test pour le pipeline avec agr√©gation automatique.
Teste le pipeline end-to-end et v√©rifie que l'agr√©gation est automatique.
"""
import subprocess
import sys
from pathlib import Path
import logging


def setup_logging():
    """Setup logging configuration."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)


def run_command(command, description):
    """Ex√©cute une commande et log le r√©sultat."""
    logger = logging.getLogger(__name__)
    logger.info(f"üöÄ {description}")
    logger.info(f"Commande: {command}")

    try:
        result = subprocess.run(command, shell=True,
                                capture_output=True, text=True)

        if result.returncode == 0:
            logger.info(f"‚úÖ {description} - Succ√®s")
            if result.stdout:
                logger.info(f"Sortie: {result.stdout.strip()}")
        else:
            logger.error(f"‚ùå {description} - √âchec")
            logger.error(f"Erreur: {result.stderr.strip()}")
            return False

    except Exception as e:
        logger.error(f"‚ùå {description} - Exception: {e}")
        return False

    return True


def test_pipeline_with_aggregation():
    """Teste le pipeline avec agr√©gation automatique."""
    logger = logging.getLogger(__name__)

    print("\n" + "="*80)
    print("üöÄ TEST PIPELINE AVEC AGR√âGATION AUTOMATIQUE")
    print("="*80)

    # Test 1: Pipeline complet avec agr√©gation automatique
    logger.info("\nüìã Test 1: Pipeline complet avec agr√©gation automatique")

    pipeline_cmd = [
        "python3", "scripts/run_pipeline.py",
        "--dataset", "poc_test_aggregation",
        "--batch-size", "1",
        "--videos-per-account", "3",
        "--max-total-videos", "6",
        "--feature-system", "modular",
        "--feature-set", "comprehensive"
    ]

    success = run_command(" ".join(pipeline_cmd), "Pipeline avec agr√©gation")

    if not success:
        logger.error("‚ùå Le pipeline a √©chou√©. Arr√™t du test.")
        return False

    # Test 2: V√©rification de l'agr√©gation automatique
    logger.info("\nüìã Test 2: V√©rification de l'agr√©gation automatique")

    dataset_dir = Path("data/dataset_poc_test_aggregation")
    features_dir = dataset_dir / "features"
    aggregated_file = features_dir / "aggregated_comprehensive.csv"

    if aggregated_file.exists():
        logger.info(f"‚úÖ Fichier agr√©g√© trouv√©: {aggregated_file}")

        # Analyser le fichier agr√©g√©
        try:
            import csv
            with open(aggregated_file, 'r') as f:
                reader = csv.reader(f)
                headers = next(reader)
                rows = list(reader)

            logger.info(f"üìä Contenu du fichier agr√©g√©:")
            logger.info(f"   ‚Ä¢ Headers: {len(headers)} colonnes")
            logger.info(f"   ‚Ä¢ Lignes: {len(rows)} vid√©os")

            # V√©rifier la colonne account_name
            if 'account_name' in headers:
                account_col_idx = headers.index('account_name')
                accounts = set(row[account_col_idx]
                               for row in rows if len(row) > account_col_idx)
                logger.info(f"   ‚Ä¢ Comptes: {', '.join(accounts)}")

                # Compter par compte
                account_counts = {}
                for row in rows:
                    if len(row) > account_col_idx:
                        account = row[account_col_idx]
                        account_counts[account] = account_counts.get(
                            account, 0) + 1

                logger.info(f"   ‚Ä¢ Vid√©os par compte:")
                for account, count in account_counts.items():
                    logger.info(f"     - {account}: {count} vid√©os")

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'analyse du fichier agr√©g√©: {e}")
            return False
    else:
        logger.error(f"‚ùå Fichier agr√©g√© manquant: {aggregated_file}")
        return False

    # Test 3: V√©rification de la structure compl√®te
    logger.info("\nüìã Test 3: V√©rification de la structure compl√®te")

    files_to_check = [
        features_dir / "aggregated_comprehensive.csv",
        dataset_dir / "gemini_analysis",
        dataset_dir / "batch_*.json"
    ]

    all_files_exist = True
    for file_pattern in files_to_check:
        if file_pattern.name == "batch_*.json":
            batch_files = list(dataset_dir.glob("batch_*.json"))
            if batch_files:
                logger.info(f"‚úÖ Fichiers batch trouv√©s: {len(batch_files)}")
            else:
                logger.error(f"‚ùå Aucun fichier batch trouv√©")
                all_files_exist = False
        elif file_pattern.name == "gemini_analysis":
            if file_pattern.exists():
                analysis_dirs = list(file_pattern.iterdir())
                logger.info(
                    f"‚úÖ Dossiers d'analyse trouv√©s: {len(analysis_dirs)}")
            else:
                logger.error(f"‚ùå Dossier d'analyse non trouv√©")
                all_files_exist = False
        else:
            if file_pattern.exists():
                logger.info(f"‚úÖ Fichier trouv√©: {file_pattern}")
            else:
                logger.error(f"‚ùå Fichier manquant: {file_pattern}")
                all_files_exist = False

    if not all_files_exist:
        logger.error("‚ùå Certains fichiers sont manquants.")
        return False

    # Test 4: V√©rification des features individuelles
    logger.info("\nüìã Test 4: V√©rification des features individuelles")

    feature_files = list(features_dir.glob("*_features_comprehensive.csv"))
    if feature_files:
        logger.info(
            f"‚úÖ Fichiers de features individuels trouv√©s: {len(feature_files)}")
        for file_path in feature_files:
            account_name = file_path.stem.replace(
                '_features_comprehensive', '')
            logger.info(f"   ‚Ä¢ {account_name}: {file_path}")
    else:
        logger.warning("‚ö†Ô∏è Aucun fichier de features individuel trouv√©")

    # R√©sum√© final
    print("\n" + "="*80)
    print("‚úÖ TEST PIPELINE AVEC AGR√âGATION TERMIN√â")
    print("="*80)
    print("üìä R√©sultats:")
    print("   ‚Ä¢ Pipeline: ‚úÖ Fonctionnel")
    print("   ‚Ä¢ Agr√©gation automatique: ‚úÖ Fonctionnelle")
    print("   ‚Ä¢ Fichiers: ‚úÖ G√©n√©r√©s")
    print("   ‚Ä¢ Structure: ‚úÖ Correcte")
    print("\nüéØ Prochaines √©tapes:")
    print("   1. Analyser les donn√©es avec: python3 scripts/analyze_existing_data.py --dataset-dir data/dataset_poc_test_aggregation")
    print("   2. Cr√©er un mod√®le baseline")
    print("   3. Augmenter le dataset pour l'entra√Ænement")

    return True


def main():
    """Main function."""
    logger = setup_logging()

    try:
        success = test_pipeline_with_aggregation()
        return 0 if success else 1

    except Exception as e:
        logger.error(f"‚ùå Erreur lors du test: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
